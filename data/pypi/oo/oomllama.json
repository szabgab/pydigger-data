{
  "name": "oomllama",
  "version": "0.4.0",
  "summary": "Efficient LLM inference with .oom format - 2x smaller than GGUF. Dual GPU support, RoPE, KV-Cache & Flash Attention! pip install oomllama[cuda]",
  "license": "MIT",
  "license_expression": null,
  "home_page": "https://humotica.nl",
  "home_page_source": "project_urls.homepage",
  "maintainer": null,
  "author": null,
  "repository": "https://github.com/humotica/oomllama",
  "repository_source": "project_urls.repository",
  "download": null,
  "download_source": null,
  "pub_date": 1770007714,
  "project_urls": {
    "Homepage": "https://humotica.nl",
    "Documentation": "https://humotica.nl/docs/oomllama",
    "HuggingFace Models": "https://huggingface.co/jaspervandemeent",
    "Repository": "https://github.com/humotica/oomllama",
    "Bug Tracker": "https://github.com/humotica/oomllama/issues"
  },
  "has_github_actions": false,
  "has_gitlab_pipeline": null,
  "has_dependabot": false,
  "has_pyproject_toml": null,
  "has_setup_py": null,
  "has_setup_cfg": null
}