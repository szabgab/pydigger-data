{
  "name": "lk-moe",
  "version": "1.5.8",
  "summary": "lk_moe is a special NUMA extension of vllm that makes full use of CPU and memory resources, reduces GPU memory requirements, and features an efficient GPU parallel and NUMA parallel architecture, supporting hybrid inference for MOE large models.",
  "license": null,
  "license_expression": null,
  "home_page": null,
  "home_page_source": null,
  "maintainer": null,
  "author": null,
  "repository": null,
  "repository_source": null,
  "download": null,
  "download_source": null,
  "pub_date": 1769190911,
  "project_urls": {},
  "has_github_actions": null,
  "has_gitlab_pipeline": null,
  "has_dependabot": null,
  "has_pyproject_toml": null,
  "has_setup_py": null,
  "has_setup_cfg": null
}