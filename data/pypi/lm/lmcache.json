{
  "name": "lmcache",
  "version": "0.3.13",
  "summary": "A LLM serving engine extension to reduce TTFT and increase throughput, especially under long-context scenarios.",
  "license": null,
  "license_expression": "Apache-2.0",
  "home_page": "https://docs.lmcache.ai",
  "home_page_source": "project_urls.homepage",
  "maintainer": null,
  "author": null,
  "repository": "https://github.com/LMCache/LMCache",
  "repository_source": "project_urls.source",
  "download": null,
  "download_source": null,
  "pub_date": 1769726018,
  "project_urls": {
    "homepage": "https://docs.lmcache.ai",
    "issues": "https://github.com/LMCache/LMCache",
    "source": "https://github.com/LMCache/LMCache"
  },
  "has_github_actions": true,
  "has_gitlab_pipeline": null,
  "has_dependabot": true,
  "has_pyproject_toml": true,
  "has_setup_py": true,
  "has_setup_cfg": false
}