{
  "name": "vllm",
  "version": "0.14.0",
  "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "license": null,
  "license_expression": "Apache-2.0",
  "home_page": "https://github.com/vllm-project/vllm",
  "maintainer": null,
  "author": "vLLM Team",
  "repository": "https://github.com/vllm-project/vllm",
  "repository_source": "project_urls.homepage",
  "pub_date": 1768906200,
  "project_urls": {
    "Homepage": "https://github.com/vllm-project/vllm",
    "Slack": "https://slack.vllm.ai/",
    "Documentation": "https://docs.vllm.ai/en/latest/"
  },
  "has_github_actions": true,
  "has_gitlab_pipeline": null,
  "has_dependabot": true
}