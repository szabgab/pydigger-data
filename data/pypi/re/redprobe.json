{
  "name": "redprobe",
  "version": "0.1.1",
  "summary": "A defensive security tool for hardening AI systems. Define YAML-based test cases to systematically probe LLMs for jailbreaks, prompt injections, biases, harmful content generation, data leakage, and policy violations before attackers find them. Compatible with any OpenAI-style API endpoint.",
  "license": "BUSL 1.1",
  "license_expression": null,
  "home_page": null,
  "home_page_source": null,
  "maintainer": null,
  "author": null,
  "repository": null,
  "repository_source": null,
  "download": null,
  "download_source": null,
  "pub_date": 1770107230,
  "project_urls": {},
  "has_github_actions": null,
  "has_gitlab_pipeline": null,
  "has_dependabot": null,
  "has_pyproject_toml": null,
  "has_setup_py": null,
  "has_setup_cfg": null
}